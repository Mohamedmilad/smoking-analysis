{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasting blood sugar', 'serum creatinine', 'Urine protein', 'age', 'weight(kg)', 'Cholesterol', 'eyesight(left)', 'eyesight(right)', 'hearing(left)', 'triglyceride']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_1 = 7479 #change to first student id\n",
    "id_2 = 7839 #change to second student id\n",
    "id_3 = 0000 #change to third student id \"leave 0000 if team of 2\"\n",
    "random_seed = id_1+id_2+id_3\n",
    "random.seed(random_seed)\n",
    "data_path=\"data.csv\"#replace with data path\n",
    "output_path=\"your_data.csv\"#replace with output data path\n",
    "\n",
    "all_data=pd.read_csv(data_path) \n",
    "all_columns = all_data.columns.tolist()\n",
    "\n",
    "target_column = 'smoking'  \n",
    "\n",
    "all_columns.remove(target_column)\n",
    "\n",
    "selected_columns = random.sample(all_columns, 10)\n",
    "\n",
    "print(selected_columns) #MUST BE PRINTED\n",
    "selected_columns = np.append(selected_columns, target_column)\n",
    "sample_df = all_data[selected_columns].copy()\n",
    "sample_df.to_csv(output_path)   #From HERE YOU CAN SPLIT FOR TRAIN ,VALID AND TEST\n",
    "df=sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier()),\n",
       "             param_grid=[{'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                          'base_estimator__max_depth': [1, 2, 3],\n",
       "                          'learning_rate': [0.01, 0.1, 0.5, 1],\n",
       "                          'n_estimators': [50, 100, 200]}],\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRID SEARCH \n",
    "# param_grid = [\n",
    "#   {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "#   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "#  ]\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X=df.iloc[:,0:10]\n",
    "Y=df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [50, 100, 200],\n",
    "                    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "                    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "                    'base_estimator__max_depth': [1, 2, 3]}]\n",
    "\n",
    "# cv number of times to run all possible answers\n",
    "clf = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), tuned_parameters, cv=5,scoring='f1_macro')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'learning_rate': 0.5,\n",
       " 'base_estimator__max_depth': 3,\n",
       " 'algorithm': 'SAMME.R'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_\n",
    "clf.cv_results_\n",
    "clf.cv_results_.keys()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "for param, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score']):\n",
    "    print(param, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier()),\n",
       "                   param_distributions=[{'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                                         'base_estimator__max_depth': [1, 2, 3],\n",
       "                                         'learning_rate': [0.01, 0.1, 0.5, 1],\n",
       "                                         'n_estimators': [50, 100, 200]}],\n",
       "                   scoring='f1_macro')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized SEARCH \n",
    "import scipy\n",
    "# n_iter-> 10 random combinations of the hyperparamaters\n",
    "clf = RandomizedSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), tuned_parameters, cv=5,n_iter=10,scoring='f1_macro')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.75     13359\n",
      "           1       0.67      0.75      0.71     10529\n",
      "\n",
      "    accuracy                           0.73     23888\n",
      "   macro avg       0.73      0.73      0.73     23888\n",
      "weighted avg       0.73      0.73      0.73     23888\n",
      "\n",
      "{'n_estimators': 50, 'learning_rate': 0.5, 'base_estimator__max_depth': 2, 'algorithm': 'SAMME.R'} 0.7236740140046601\n",
      "{'n_estimators': 200, 'learning_rate': 1, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME.R'} 0.7241598571497487\n",
      "{'n_estimators': 200, 'learning_rate': 0.5, 'base_estimator__max_depth': 2, 'algorithm': 'SAMME'} 0.7157972460544679\n",
      "{'n_estimators': 100, 'learning_rate': 0.1, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME.R'} 0.7262459435869651\n",
      "{'n_estimators': 100, 'learning_rate': 0.01, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME.R'} 0.7112505835210439\n",
      "{'n_estimators': 200, 'learning_rate': 0.5, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME.R'} 0.7287380702794156\n",
      "{'n_estimators': 50, 'learning_rate': 0.1, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME.R'} 0.7244366856107926\n",
      "{'n_estimators': 200, 'learning_rate': 0.1, 'base_estimator__max_depth': 2, 'algorithm': 'SAMME'} 0.715414536056255\n",
      "{'n_estimators': 200, 'learning_rate': 0.01, 'base_estimator__max_depth': 2, 'algorithm': 'SAMME.R'} 0.7092178757952475\n",
      "{'n_estimators': 50, 'learning_rate': 0.5, 'base_estimator__max_depth': 3, 'algorithm': 'SAMME'} 0.7185820955248107\n"
     ]
    }
   ],
   "source": [
    "clf.best_params_\n",
    "clf.cv_results_\n",
    "clf.cv_results_.keys()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "for param, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score']):\n",
    "    print(param, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:56<00:00,  5.93s/trial, best loss: -0.7272689216342934]\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Test Accuracy with Best Hyperparameters: 0.7283991962491627\n"
     ]
    }
   ],
   "source": [
    "# bayesian utilize the results from the previous step to decide which hyperparameter to evaluate next\n",
    "# acquisition function search for the next set of hyperparameters the TPE algorithm utilizes this historical information to decide where to sample the next set of hyperparameters.\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "X=df.iloc[:,0:10]\n",
    "Y=df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# the objective function is the function that measures the performance of a machine learning model with a given set of hyperparameters\n",
    "def objective(params):\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 1, 2, 3, 4, 5]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4])\n",
    "}\n",
    "# The Trials object keeps track of the hyperparameter configurations and their corresponding results across multiple iterations\n",
    "# Perform hyperparameter optimization fmin best hyperparameter with the smallest loss \n",
    "# (TPE Tree of Parzen Estimators) search algorithm ->acquisition function\n",
    "# max value-> max number of evaluations\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = hyperopt.space_eval(space, best)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = RandomForestClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy with Best Hyperparameters:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Accuracy: 0.728985264567984\n"
     ]
    }
   ],
   "source": [
    "# FINAL SYSTEM \n",
    "boosting_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),algorithm='SAMME.R',learning_rate=0.5,n_estimators=200,random_state=42)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "y_pred_boosting = boosting_model.predict(X_test)\n",
    "accuracy_boosting = accuracy_score(y_test, y_pred_boosting)\n",
    "print(f\"Boosting Accuracy: {accuracy_boosting}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
